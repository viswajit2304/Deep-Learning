{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_feature_maps_cl2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, stride=1, padding=1) # CL1\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2) # Mean Pooling Layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=num_feature_maps_cl2, kernel_size=3, stride=1, padding=1) # CL2\n",
    "        self.fc1 = nn.Linear(num_feature_maps_cl2 * 32 * 32, 5) # Assuming the input images are 32x32, adjust if different\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) # CL1 -> PL1\n",
    "        x = self.pool(F.relu(self.conv2(x))) # CL2 -> PL2\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, '')\n",
    "\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "valid_dir = os.path.join(data_dir, 'valid')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = datasets.ImageFolder(root=valid_dir, transform=transform)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "num_feature_maps_cl2 = 4 # Example value, this is a hyperparameter\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleCNN(num_feature_maps_cl2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] loss: 1.3913\n",
      "[Epoch 2] loss: 1.0493\n",
      "[Epoch 3] loss: 0.7186\n",
      "[Epoch 4] loss: 0.4413\n",
      "[Epoch 5] loss: 0.3043\n",
      "[Epoch 6] loss: 0.2641\n",
      "[Epoch 7] loss: 0.1705\n",
      "[Epoch 8] loss: 0.1163\n",
      "[Epoch 9] loss: 0.1326\n",
      "[Epoch 10] loss: 0.1067\n",
      "[Epoch 11] loss: 0.0923\n",
      "[Epoch 12] loss: 0.1059\n",
      "[Epoch 13] loss: 0.1099\n",
      "[Epoch 14] loss: 0.1387\n",
      "[Epoch 15] loss: 0.0751\n",
      "[Epoch 16] loss: 0.0430\n",
      "[Epoch 17] loss: 0.0170\n",
      "[Epoch 18] loss: 0.0067\n",
      "[Epoch 19] loss: 0.0032\n",
      "[Epoch 20] loss: 0.0019\n",
      "[Epoch 21] loss: 0.0015\n",
      "[Epoch 22] loss: 0.0012\n",
      "[Epoch 23] loss: 0.0010\n",
      "[Epoch 24] loss: 0.0009\n",
      "Early stopping at epoch 25\n"
     ]
    }
   ],
   "source": [
    "# Define the early stopping threshold\n",
    "threshold = 0.0001  # Define a suitable threshold for your problem\n",
    "previous_loss = float('inf')  # Initialize with infinity to ensure the first epoch is not stopped\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate the average loss for this epoch\n",
    "    average_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # Check if the change in loss is below the threshold\n",
    "    if abs(previous_loss - average_loss) < threshold:\n",
    "        print(f'Early stopping at epoch {epoch + 1}')\n",
    "        break\n",
    "    \n",
    "    # Update the previous loss\n",
    "    previous_loss = average_loss\n",
    "\n",
    "    print(f'[Epoch {epoch + 1}] loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 100.00%\n",
      "Validation accuracy: 48.60%\n",
      "Test accuracy: 45.80%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on validation data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Store labels and predictions\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(f'Train accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "# # Compute the confusion matrix\n",
    "# cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# # Plot the confusion matrix as a heatmap\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.title('Confusion Matrix for train')\n",
    "# plt.show()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in valid_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Validation accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Testing the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(f'Test accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "# # Compute the confusion matrix\n",
    "# cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# # Plot the confusion matrix as a heatmap\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.title('Confusion Matrix for Test')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
