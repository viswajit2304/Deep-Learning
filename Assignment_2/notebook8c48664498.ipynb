{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2628777,"sourceType":"datasetVersion","datasetId":1598327},{"sourceId":8518972,"sourceType":"datasetVersion","datasetId":5086257}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import nn\nfrom transformers import AlbertModel, AlbertTokenizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.translate.bleu_score import sentence_bleu, corpus_bleu\nfrom datasets import load_dataset, DatasetDict\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:55:35.129634Z","iopub.execute_input":"2024-05-26T16:55:35.130061Z","iopub.status.idle":"2024-05-26T16:55:45.763861Z","shell.execute_reply.started":"2024-05-26T16:55:35.130026Z","shell.execute_reply":"2024-05-26T16:55:45.762529Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"# Load datasets from CSV files\ntrain_dataset = load_dataset('csv', data_files='/kaggle/input/myfiles/team16_ta_train.csv', split='train')\nvalidation_dataset = load_dataset('csv', data_files='/kaggle/input/myfiles/team16_ta_valid.csv', split='train')\ntest_dataset = load_dataset('csv', data_files='/kaggle/input/myfiles/team16_ta_test.csv', split='train')\n\n# Create a DatasetDict\ndataset = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": validation_dataset,\n    \"test\": test_dataset\n})","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:57:36.297170Z","iopub.execute_input":"2024-05-26T16:57:36.297593Z","iopub.status.idle":"2024-05-26T16:58:37.524517Z","shell.execute_reply.started":"2024-05-26T16:57:36.297547Z","shell.execute_reply":"2024-05-26T16:58:37.523354Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58c9a4eb537c4068b25eb79652f28c5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0879cd17d434d1da98730aa6b269d21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9f2fe270b5641889ea20cc5f5e32428"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Output Embeddings","metadata":{}},{"cell_type":"code","source":"tokenizer = AlbertTokenizer.from_pretrained('/kaggle/input/ai4bharat-indic-bert')\nmodel = AlbertModel.from_pretrained('/kaggle/input/ai4bharat-indic-bert')\n\ndef tokenize_ta(text):\n    return tokenizer(text, padding=True, return_tensors='pt')\n\ndef embedding_ta(tokens:dict|torch.Tensor, model=model):   \n    with torch.no_grad():\n        if isinstance(tokens, torch.Tensor):\n            output = model(tokens)\n        else:\n            output = model(**tokens)\n\n    return output.last_hidden_state","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:58:44.326933Z","iopub.execute_input":"2024-05-26T16:58:44.327685Z","iopub.status.idle":"2024-05-26T16:58:47.522322Z","shell.execute_reply.started":"2024-05-26T16:58:44.327648Z","shell.execute_reply":"2024-05-26T16:58:47.521272Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Building Vocabolary for Target Language","metadata":{}},{"cell_type":"code","source":"def build_vocab_ta():\n    temp = {0: 3}\n    for sentence in tqdm(dataset['train']['target']):\n        for token in tokenize_ta(sentence)['input_ids'][0]:\n            if temp.get(token.item()) is None:\n                temp[token.item()] = 1\n            else:\n                temp[token.item()] = temp[token.item()] + 1\n    \n    vocab = set()\n    for tk in temp.keys():\n        if temp[tk] > 1:\n            vocab.add(tk)\n            \n    return torch.tensor(np.sort(list(vocab)))\n\nvocab_ta = build_vocab_ta()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:59:00.371175Z","iopub.execute_input":"2024-05-26T16:59:00.371610Z","iopub.status.idle":"2024-05-26T16:59:49.652407Z","shell.execute_reply.started":"2024-05-26T16:59:00.371575Z","shell.execute_reply":"2024-05-26T16:59:49.651348Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"100%|██████████| 70000/70000 [00:49<00:00, 1426.69it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def find_neighbors(tensor, input_number):\n    sorted_list = tensor.tolist()\n    position = 0\n    for i, num in enumerate(sorted_list):\n        if num >= input_number:\n            position = i\n            break\n    return sorted_list[max(0, position-2):min(len(sorted_list), position+2)]\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T17:18:46.439081Z","iopub.execute_input":"2024-05-26T17:18:46.439523Z","iopub.status.idle":"2024-05-26T17:18:46.447049Z","shell.execute_reply.started":"2024-05-26T17:18:46.439490Z","shell.execute_reply":"2024-05-26T17:18:46.445708Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"sentence = 'என் நினைவுகள் உதிர்ந்துவிட்டன.'\ntks = tokenize_ta(sentence)\ntks['input_ids']","metadata":{"execution":{"iopub.status.busy":"2024-05-26T17:00:44.331549Z","iopub.execute_input":"2024-05-26T17:00:44.331930Z","iopub.status.idle":"2024-05-26T17:00:44.344585Z","shell.execute_reply.started":"2024-05-26T17:00:44.331901Z","shell.execute_reply":"2024-05-26T17:00:44.343213Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"tensor([[     2,    396,  13621,  18248, 120345,  13143,    330,   7388, 140713,\n              9,      3]])"},"metadata":{}}]},{"cell_type":"code","source":"find_neighbors(vocab_ta, 120345)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T17:22:57.813904Z","iopub.execute_input":"2024-05-26T17:22:57.814338Z","iopub.status.idle":"2024-05-26T17:22:57.822640Z","shell.execute_reply.started":"2024-05-26T17:22:57.814304Z","shell.execute_reply":"2024-05-26T17:22:57.821621Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[120237, 120258, 120345, 120382]"},"metadata":{}}]},{"cell_type":"code","source":"rks = torch.tensor([    2,    398,  13621,  18248, 120382,  13143,    326,   7388, 140713,  9,      3])\ntokenizer.decode(rks)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T17:23:05.904560Z","iopub.execute_input":"2024-05-26T17:23:05.904994Z","iopub.status.idle":"2024-05-26T17:23:05.914132Z","shell.execute_reply.started":"2024-05-26T17:23:05.904931Z","shell.execute_reply":"2024-05-26T17:23:05.912899Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"\"[CLS]'நினைவுகள சீதாநdவிடடன.[SEP]\""},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_map(vocab):\n    vocab_map = dict()\n    model_input = {\n        'token_type_ids':   torch.tensor(0).reshape(1,1),\n        'attention_mask':   torch.tensor(1).reshape(1,1)\n    }\n    for token in tqdm(vocab):\n        model_input['input_ids'] = torch.tensor(token.item()).reshape(1,1)\n        with torch.no_grad():\n            vocab_map[token.item()] = model(**model_input).last_hidden_state\n    \n    return vocab_map\n\n# dict with each token as key and respective embedding as value\nmap_ta = build_map(vocab_ta)\nmap_ta[0] = torch.zeros(1, 1, 768)","metadata":{},"execution_count":10,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 8074/8074 [03:59<00:00, 33.68it/s]\n"}]},{"cell_type":"markdown","source":"## Seq2Seq Model","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.rnn = nn.LSTM(input_dim, hidden_dim, batch_first=True, dtype=torch.float32)\n\n    # input_seq : source sequence embeddings => (N, seq_len, em_size) / (seq_len, em_size)\n    def forward(self, input_seq):\n        output, curr_state = self.rnn(input_seq)\n        return curr_state\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.rnn = nn.LSTM(input_dim, hidden_dim, batch_first=True, dtype=torch.float32)\n        self.linear = nn.Linear(hidden_dim, output_dim, dtype=torch.float32)\n        self.softmax = nn.Softmax(dim=1)\n    \n    # y : [mostly] prev. (expected) word embedding => (N, em_size) / (1, em_size)\n    def forward(self, y, prev_state):\n        output, curr_state = self.rnn(y, prev_state)\n        prediction = self.softmax(self.linear(output))\n        return prediction, curr_state","metadata":{},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class Seq2SeqModel(nn.Module):\n    def __init__(self, src_em_dim, hidden_dim, tgt_em_dim, tgt_dim):\n        super().__init__()\n        self.src_em_dim = src_em_dim\n        self.hidden_dim = hidden_dim\n        self.tgt_em_dim = tgt_em_dim\n        self.tgt_dim = tgt_dim\n        self.encoder = Encoder(src_em_dim, hidden_dim)\n        self.decoder = Decoder(tgt_em_dim, hidden_dim, tgt_dim)\n\n    # source : tensor of embeddings of tokens\n    # target : tensor of tokens only => (N, seq_len)\n    def forward(self, source, target=None):\n        batch_size = source.shape[0]\n        target_len = 1000 if target is None else target.shape[1]\n        \n        last_encoder_state = self.encoder(source)\n\n        outputs = []\n        prev_state = last_encoder_state\n\n        # should be (N, 1, em_size)\n        decoder_input = torch.tile(map_ta[2], (batch_size, 1, 1))\n        \n        for t in range(1, target_len):\n            decoder_output, state = self.decoder(decoder_input, prev_state)\n            outputs.append(decoder_output)\n            prev_state = state\n\n            if self.training:\n                temp1 = [map_ta.get(tk.item(), map_ta[2]) for tk in target[:, t]]\n                decoder_input = torch.concat(temp1)\n            else:\n                decoder_input = torch.concat([map_ta.get(vocab_ta[torch.argmax(y)].item(), map_ta[2]) for y in decoder_output])\n            \n        return torch.concat(outputs, dim=1).to(device)","metadata":{},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"num_epochs = 5\nlearning_rate = 0.001\nbatch_size = 64\n\nen_vocab_size = 0 # not neccesary since we are not constructing any vocabolary for english and it is not needed\nen_embedding_size = input_dim\nta_vocab_size = len(vocab_ta)\nta_embedding_size = 768\n\nhidden_dim = en_embedding_size * 2\n\nmachine = Seq2SeqModel(en_embedding_size, hidden_dim, ta_embedding_size, ta_vocab_size)\n\nmachine.encoder.to(device)\nmachine.decoder.to(device)\nmachine.to(device)\n\noptimizer = torch.optim.Adam(machine.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\n\ntrain_dataset.set_format(type='torch', columns=['source', 'target'])\nvalidation_dataset.set_format(type='torch', columns=['source', 'target'])\ntest_dataset.set_format(type='torch', columns=['source', 'target'])\n\ndef collate_fn(example_list: list):\n    source_list = [example['source'] for example in example_list]\n    target_list = [example['target'] for example in example_list]\n\n    source_tensor = embedding_en(tokenize_en(source_list))\n    target_tensor = tokenize_ta(target_list)['input_ids']\n\n    return source_tensor, target_tensor\n","metadata":{},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    \n    machine.train()\n\n    train_iterator = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n    tqdm_iterator = tqdm(train_iterator)\n    for source, target in tqdm_iterator:\n\n        optimizer.zero_grad()\n\n        outputs = machine(source, target)\n        \n        target_one_hot = nn.functional.one_hot(token_to_id(target[:, 1:]), num_classes=machine.tgt_dim).float()\n        loss = criterion(outputs, target_one_hot)\n\n        loss.backward()\n\n        optimizer.step()\n        \n    machine.eval()\n\n    valid_iterator = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n    avg_loss = 0\n    for source, target in tqdm(valid_iterator):\n        outputs = machine(source, target)\n        target_one_hot = nn.functional.one_hot(token_to_id(target[:, 1:]), num_classes=machine.tgt_dim).float()\n        loss = criterion(outputs, target_one_hot)\n        avg_loss = avg_loss + loss\n\n    avg_loss = avg_loss/len(validation_dataset)\n    print(f'Epoch [{epoch+1}] ---------------------------------------------- Loss: {avg_loss}')","metadata":{},"execution_count":22,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 1094/1094 [45:49<00:00,  2.51s/it] \n\n  9%|▊         | 27/313 [01:18<11:02,  2.32s/it]"}]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"machine.eval()","metadata":{},"execution_count":null,"outputs":[{"execution_count":154,"output_type":"execute_result","data":{"text/plain":["Seq2SeqModel(\n","  (encoder): Encoder(\n","    (rnn): LSTM(50, 100, batch_first=True)\n","  )\n","  (decoder): Decoder(\n","    (rnn): LSTM(768, 100, batch_first=True)\n","    (fc): Linear(in_features=100, out_features=8074, bias=True)\n","  )\n",")"]},"metadata":{}}]},{"cell_type":"code","source":"def decode_sentences(outputs): \n\n    def decode_sentence(Y): # Y is 2D tensor => (seq_len, vocab_size)\n        tokens = []\n        for y in Y:\n            token = vocab_ta[torch.argmax(y)].item()\n            if token == 0:\n                break\n            else:\n                tokens.append(token)\n            \n        return tokenizer.decode(tokens)\n\n    if len(outputs.shape) == 2:   \n        outputs = outputs.unsqueeze(dim=0)\n    \n    return [decode_sentence(Y) for Y in outputs]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 5\n\nen_sentence = embedding_en(tokenize_en(dataset['train']['source'][idx]))\nta_sentence = dataset['train']['target'][idx]\n\nmachine.eval()\noutputs = machine(en_sentence)\n\nta_sentence, decode_sentences(outputs)","metadata":{},"execution_count":null,"outputs":[{"execution_count":143,"output_type":"execute_result","data":{"text/plain":["('எனக்கு ஒன்றும் தோன்ற வில்லை.', ['கக'])"]},"metadata":{}}]},{"cell_type":"code","source":"for source, target in tqdm(test_dataset):\n    pass","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 10000/10000 [00:01<00:00, 8992.33it/s]\n"}]},{"cell_type":"markdown","source":"## BLEU Scores","metadata":{}},{"cell_type":"code","source":"def compute_bleu_scores():\n\n    generated_sentences = []\n    reference_sentences = []\n\n    def collate_fn(example_list: list):\n    source_list = [example['source'] for example in example_list]\n    target_list = [example['target'] for example in example_list]\n\n    source_tensor = embedding_en(tokenize_en(source_list))\n\n    return source_tensor, target_list\n\n    iterator = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn = collate_fn)\n    for source, target in tqdm(iterator):\n        reference_sentences.append([target])\n        generated_sentences.append(decode_sentences(machine(source)))\n\n    reference_tokens = [[word_tokenize(ref) for ref in refs] for refs in reference_sentences]\n    generated_tokens = [word_tokenize(gen) for gen in generated_sentences]\n\n    corpus_bleu_score_1 = corpus_bleu(reference_tokens, generated_tokens, (1, 0, 0, 0))\n    corpus_bleu_score_2 = corpus_bleu(reference_tokens, generated_tokens, (1/2, 1/2, 0, 0))\n    corpus_bleu_score_3 = corpus_bleu(reference_tokens, generated_tokens, (1/3, 1/3, 1/3, 0))\n    corpus_bleu_score_4 = corpus_bleu(reference_tokens, generated_tokens, (1/4, 1/4, 1/4, 1/4))\n\n    return {\n        'BLEU@1': corpus_bleu_score_1,\n        'BLEU@2': corpus_bleu_score_2,\n        'BLEU@3': corpus_bleu_score_3,\n        'BLEU@4': corpus_bleu_score_4\n    }\n\nbleu_scores = compute_bleu_scores()\nbleu_scores","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"  4%|▍         | 7/157 [03:02<1:05:06, 26.05s/it]\n"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[155], line 26\u001b[0m\n\u001b[0;32m     17\u001b[0m     corpus_bleu_score_4 \u001b[38;5;241m=\u001b[39m corpus_bleu(reference_tokens, generated_tokens, (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLEU@1\u001b[39m\u001b[38;5;124m'\u001b[39m: corpus_bleu_score_1,\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLEU@2\u001b[39m\u001b[38;5;124m'\u001b[39m: corpus_bleu_score_2,\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLEU@3\u001b[39m\u001b[38;5;124m'\u001b[39m: corpus_bleu_score_3,\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLEU@4\u001b[39m\u001b[38;5;124m'\u001b[39m: corpus_bleu_score_4\n\u001b[0;32m     24\u001b[0m     }\n\u001b[1;32m---> 26\u001b[0m bleu_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_bleu_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m bleu_scores\n","Cell \u001b[1;32mIn[155], line 9\u001b[0m, in \u001b[0;36mcompute_bleu_scores\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m source, target \u001b[38;5;129;01min\u001b[39;00m tqdm(iterator):\n\u001b[0;32m      8\u001b[0m     reference_sentences\u001b[38;5;241m.\u001b[39mappend([target])\n\u001b[1;32m----> 9\u001b[0m     generated_sentences\u001b[38;5;241m.\u001b[39mappend(decode_sentences(\u001b[43mmachine\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     11\u001b[0m reference_tokens \u001b[38;5;241m=\u001b[39m [[word_tokenize(ref) \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m refs] \u001b[38;5;28;01mfor\u001b[39;00m refs \u001b[38;5;129;01min\u001b[39;00m reference_sentences]\n\u001b[0;32m     12\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m [word_tokenize(gen) \u001b[38;5;28;01mfor\u001b[39;00m gen \u001b[38;5;129;01min\u001b[39;00m generated_sentences]\n","File \u001b[1;32mc:\\Users\\samk1\\Conda\\miniconda3\\envs\\rosemilk\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\samk1\\Conda\\miniconda3\\envs\\rosemilk\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[1;32mIn[108], line 34\u001b[0m, in \u001b[0;36mSeq2SeqModel.forward\u001b[1;34m(self, source, target)\u001b[0m\n\u001b[0;32m     32\u001b[0m         decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat(temp1)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m         decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([map_ta\u001b[38;5;241m.\u001b[39mget(vocab_ta[torch\u001b[38;5;241m.\u001b[39margmax(y)]\u001b[38;5;241m.\u001b[39mitem(), map_ta[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m decoder_output])\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mconcat(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n","Cell \u001b[1;32mIn[108], line 34\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     32\u001b[0m         decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat(temp1)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m         decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([map_ta\u001b[38;5;241m.\u001b[39mget(vocab_ta[\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m]\u001b[38;5;241m.\u001b[39mitem(), map_ta[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m decoder_output])\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mconcat(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}